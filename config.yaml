"""
PARAMS USED FOR TRAINING TRANSFORMER

@author: vasudevgupta
"""
dataloader:
    # don't tune it now
    num_samples: 15000 ## taking only 25000 samples for training purposes
    eng_vocab: 5776 # got this after tokenizing dataset- english
    ger_vocab: 8960 # got this after tokenizing dataset- german
    dec_max_len: 17
    en_max_len: 20

rnn_attention:
    batch_size: 64
    learning_rate: .001
    optimizer: 'rmsprop'
    epochs: 400
    ckpt_dir: 'weights/rnn_attention_ckpts'
    
    embed_size: 300
    gru_units: 128

transformers:
    warmup_steps: 4000
    optimizer: 'adam'
    learning_rate: 'schedule' # either 'schedule' or some constant
    epochs: 20
    ckpt_dir: 'weights/transformers_ckpts'
    
    k: 5 # beam width
    num_blocks: 4
    dmodel: 256
    num_heads: 4

