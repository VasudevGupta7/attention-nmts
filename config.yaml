# PARAMS USED FOR TRAINING TRANSFORMER
# @author: vasudevgupta

rnn_attention:
    learning_rate: .1
    optimizer: 'rmsprop'
    epochs: 2
    ckpt_dir: 'weights/rnn_attention_ckpts'
    
    embed_size: 100
    gru_units: 64

transformer:
    warmup_steps: 4000
    optimizer: 'adam'
    learning_rate: 'schedule' # either 'schedule' or some constant
    epochs: 2
    ckpt_dir: 'weights/transformers_ckpts'
    
    k: 5 # beam width
    num_blocks: 2
    dmodel: 256
    num_heads: 4
    
# don't tune it now
dataloader:
    num_samples: 15000 ## taking only 25000 samples for training purposes
    batch_size: 64
    eng_vocab: 5776 # got this after tokenizing dataset- english
    ger_vocab: 8960 # got this after tokenizing dataset- german
    dec_max_len: 17
    en_max_len: 20
